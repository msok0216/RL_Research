{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7cbacd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup import main1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9c84c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    list = []\n",
    "    for i in range(9):\n",
    "        r = 'Study' + str(i+1) + '!A:AP'\n",
    "        list.append(main1(r, \"1zDzD9GVoc_7nNLd7EB9LDxtshxxB4NX5t7Tn2yjmrow\"))\n",
    "    return list\n",
    "\n",
    "data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6a732c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_states(data):\n",
    "#     print(data)\n",
    "    data = data.replace('D7', 'D6D15')\n",
    "    data = data.replace('D8', 'D6D15')\n",
    "    data = data.replace('E3', 'D10')\n",
    "    data = data.replace('E1', 'D11')\n",
    "    data = data.replace('E2', 'D13')\n",
    "    data = data.replace('D18', 'D6D15D17')\n",
    "    data = data.replace('D19', 'D1D2')\n",
    "    data = data.replace('D21', 'D3D2')\n",
    "    data = data.replace('D22', 'D3D1')\n",
    "    data = data.replace('D23', 'D3D17')\n",
    "    data = data.replace('D25', 'D1D2D9')\n",
    "    data = data.replace('D18', 'D6D15D17')\n",
    "    data = data.replace('D27', 'D3D17D1')\n",
    "    data = data.replace('D28', 'D5')\n",
    "    data = data.replace('D29', 'D17D14')\n",
    "    data = data.replace('D30', 'D6')\n",
    "    data = data.replace('D31', 'D5D2')\n",
    "    data = data.replace('D32', 'D10')\n",
    "    data = data.replace('D33', 'D6')\n",
    "    data = data.replace('D34', 'D16D1')\n",
    "    data = data.replace('D36', 'D17D16')\n",
    "    data = data.replace('D37', 'D1D17')\n",
    "    data = data.replace('D38', 'D3D17D1')\n",
    "    data = data.replace('D39', 'D3D6D5')\n",
    "    data = data.replace('D40', 'D17D1')\n",
    "    data = data.replace('D41', 'D7')\n",
    "    data = data.replace('D42', 'D13')\n",
    "    data = data.replace('D43', 'D13')\n",
    "    data = data.replace('D44', 'D13')\n",
    "    return data\n",
    "\n",
    "# def reduce_states(data):\n",
    "#     data = data.replace('D20', 'D1')\n",
    "#     data = data.replace('D1', 'D3')\n",
    "#     data = data.replace('D5', 'D4')\n",
    "#     data = data.replace('D46', 'D5')\n",
    "#     data = data.replace('D17', 'D6')\n",
    "#     data = data.replace('D6', 'D7')\n",
    "#     data = data.replace('D9', 'D7')\n",
    "#     data = data.replace('D10', 'D7')\n",
    "#     data = data.replace('D11', 'D7')\n",
    "#     data = data.replace('D13', 'D7')\n",
    "#     data = data.replace('D14', 'D7')\n",
    "#     data = data.replace('D15', 'D7')\n",
    "#     data = data.replace('D16', 'D7')\n",
    "#     data = data.replace('D24', 'D7')\n",
    "#     data = data.replace('D26', 'D7')\n",
    "#     data = data.replace('D45', 'D7')\n",
    "#     data = data.replace('D35', 'D8')\n",
    "#     return data\n",
    "    \n",
    "\n",
    "\n",
    "def process_ide(list, idx):\n",
    "    first_speaker = []\n",
    "    second_speaker = []\n",
    "\n",
    "    curr_speaker = ' '\n",
    "    is_firstSpeaker = False\n",
    "    temp = list[idx][['Speaker','IDE State']].values\n",
    "    # temp = list[idx].values\n",
    "    # print(temp)\n",
    "    for row in temp:\n",
    "        if row[0] != curr_speaker:\n",
    "            curr_speaker = row[0]\n",
    "            is_firstSpeaker = not is_firstSpeaker\n",
    "            if is_firstSpeaker:\n",
    "                first_speaker.append([])\n",
    "            else:\n",
    "                second_speaker.append([])\n",
    "        \n",
    "        # for i in range(len(row)):\n",
    "#         print(row[1])\n",
    "        \n",
    "        row[1] = change_states(row[1])\n",
    "        temp = row[1].split('D')\n",
    "\n",
    "        if is_firstSpeaker: first_speaker[-1].extend(temp[1:])\n",
    "        else: second_speaker[-1].extend(temp[1:])\n",
    "    \n",
    "    # print(first_speaker)\n",
    "    return first_speaker,second_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3e5eb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ides = []\n",
    "for i in range(9):\n",
    "    ides.append(process_ide(data, i))\n",
    "    \n",
    "# for i in ides:\n",
    "#     print(len(i[0]), len(i[1]))\n",
    "# for i in ides[4]:\n",
    "#     if len(i) == 0: i = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "29122bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {}\n",
    "map[2] = 2\n",
    "map[3] = 3\n",
    "map[0] = 0\n",
    "map[20] = 1\n",
    "map[1] = 3\n",
    "map[5] = 4\n",
    "map[46] = 5\n",
    "map[17] = 6\n",
    "map[6] = 7\n",
    "map[9] = 7\n",
    "map[10] = 7\n",
    "map[11] = 7\n",
    "map[13] = 7\n",
    "map[14] = 7\n",
    "map[15] = 7\n",
    "map[16] = 7\n",
    "map[24] = 7\n",
    "map[26] = 7\n",
    "map[45] = 7\n",
    "map[35] = 8\n",
    "def sort(list):\n",
    "    idx = 0\n",
    "    input = []\n",
    "    output = []\n",
    "    # first speaker = 0, second speaker = 1\n",
    "    for i in list:\n",
    "        idx+=1\n",
    "        count = 0\n",
    "        for j in range(max(len(i[0]), len(i[1]))):\n",
    "            count+= 1\n",
    "            if j < len(i[1]):\n",
    "                input.append(i[0][j])\n",
    "                output.append(i[1][j][0])\n",
    "#             for i in range(len(input[-1])): input[-1][i] = int(input[-1][i])\n",
    "            while len(input[-1]) < 21: input[-1].append(0)\n",
    "            if j+1 < len(i[0]):\n",
    "                input.append(i[1][j])\n",
    "                output.append(i[0][j+1][0])\n",
    "#             for i in range(len(input[-1])): input[-1][i] = int(input[-1][i])\n",
    "            while len(input[-1]) < 21: input[-1].append(0)\n",
    "    return input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "17a4548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ide_input, ide_output = sort(ides)\n",
    "for i in range(len(ide_input)):\n",
    "    for j in range(len(ide_input[i])):\n",
    "        ide_input[i][j] = int(ide_input[i][j])\n",
    "for i in range(len(ide_output)):\n",
    "    ide_output[i] = int(ide_output[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a95c524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s - loss: 13.6360 - accuracy: 0.0913 - 341ms/epoch - 49ms/step\n",
      "7/7 - 0s - loss: 17.6556 - accuracy: 0.0817 - 56ms/epoch - 8ms/step\n",
      "7/7 - 0s - loss: 11.6915 - accuracy: 0.0865 - 73ms/epoch - 10ms/step\n",
      "7/7 - 0s - loss: 23.9163 - accuracy: 0.0769 - 80ms/epoch - 11ms/step\n",
      "7/7 - 0s - loss: 14.1234 - accuracy: 0.0577 - 58ms/epoch - 8ms/step\n",
      "7/7 - 0s - loss: 12.0989 - accuracy: 0.0769 - 79ms/epoch - 11ms/step\n",
      "7/7 - 0s - loss: 10.5254 - accuracy: 0.1490 - 80ms/epoch - 11ms/step\n",
      "7/7 - 0s - loss: 9.5697 - accuracy: 0.0817 - 65ms/epoch - 9ms/step\n",
      "7/7 - 0s - loss: 17.4822 - accuracy: 0.0769 - 77ms/epoch - 11ms/step\n",
      "7/7 - 0s - loss: 14.2318 - accuracy: 0.1058 - 97ms/epoch - 14ms/step\n",
      "Cross-validated accuracy: 0.08846153952181339\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=21, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "#         tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    ])\n",
    "#     add f1 and/or loss function for the metric because of the state imbalance\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, X, y, k=10):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "#         y_train = y_train.reshape(-1)\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        score = model.evaluate(X_test, y_test, verbose=2)\n",
    "        scores.append(score[1])\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "# print()\n",
    "# Load the input data and one-hot encode it\n",
    "X = np.asarray(ide_input)\n",
    "y = np.asarray(ide_output)\n",
    "\n",
    "model = build_model()\n",
    "score = train_and_evaluate_model(model, X, y)\n",
    "print(\"Cross-validated accuracy:\", score)\n",
    "\n",
    "# confusion matrix- tells you \n",
    "# hand validation - write code that takes one hot encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "163d5df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 3, 5, 5, 5, 5, 5, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 6, 6, 6, 6, 16, 16, 16, 1, 1, 2, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 6, 6, 6, 6, 6, 15, 15, 15, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 3, 1, 1, 17, 1, 1, 1, 1, 1, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 3, 17, 17, 17, 6, 6, 6, 6, 6, 1, 3, 3, 3, 3, 3, 3, 3, 11, 11, 17, 17, 2, 2, 20, 2, 2, 2, 3, 3, 3, 15, 3, 17, 17, 17, 17, 15, 15, 15, 15, 17, 17, 17, 17, 17, 6, 6, 15, 15, 14, 15, 16, 1, 3, 6, 6, 6, 14, 14, 14, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 16, 16, 15, 15, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 11, 11, 1, 1, 3, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 16, 16, 16, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 11, 13, 13, 13, 13, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 3, 17, 17, 1, 1, 1, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 1, 1, 1, 1, 1, 16, 16, 16, 16, 16, 16, 3, 3, 3, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 3, 3, 16, 16, 3, 3, 3, 3, 3, 16, 16, 16, 16, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 17, 17, 17, 6, 6, 16, 16, 16, 16, 6, 6, 6, 6, 1, 6, 6, 6, 6, 16, 16, 16, 16, 16, 15, 6, 6, 6, 6, 6, 6, 3, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 14, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 6, 6, 6, 6, 1, 1, 3, 3, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1, 11, 11, 11, 11, 11, 11, 11, 13, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 6, 6, 6, 6, 2, 2, 2, 15, 15, 15, 3, 6, 6, 6, 6, 1, 16, 16, 16, 16, 16, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 16, 16, 3, 3, 3, 1, 26, 2, 3, 3, 3, 3, 3, 3, 3, 16, 3, 5, 5, 5, 3, 16, 3, 16, 17, 14, 6, 6, 14, 14, 17, 17, 17, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 15, 15, 17, 3, 15, 16, 15, 6, 17, 17, 17, 17, 17, 17, 17, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 1, 1, 1, 1, 1, 15, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 3, 3, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 3, 9, 6, 6, 6, 6, 6, 1, 3, 3, 3, 3, 10, 10, 10, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 2, 2, 2, 2, 14, 14, 14, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 2, 2, 13, 13, 13, 13, 13, 13, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 16, 16, 16, 2, 2, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 16, 16, 16, 16, 16, 16, 17, 6, 15, 15, 17, 17, 5, 5, 5, 5, 35, 6, 10, 10, 10, 10, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 11, 11, 11, 16, 13, 13, 13, 13, 14, 14, 14, 14, 14, 11, 11, 11, 11, 11, 11, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 15, 14, 14, 14, 14, 14, 16, 16, 16, 16, 17, 17, 17, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 13, 13, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 1, 1, 1, 1, 1, 1, 2, 2, 17, 17, 17, 2, 2, 2, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 1, 3, 17, 1, 2, 2, 2, 3, 1, 1, 1, 1, 2, 3, 3, 14, 14, 10, 16, 16, 16, 16, 16, 17, 17, 17, 1, 3, 1, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 6, 6, 2, 2, 2, 2, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 3, 3, 15, 1, 1, 1, 1, 1, 1, 1, 1, 15, 15, 15, 15, 3, 17, 17, 17, 17, 17, 17, 6, 17, 1, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 15, 15, 15, 15, 3, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 5, 5, 5, 5, 14, 14, 3, 3, 3, 3, 17, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 24, 24, 24, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 15, 15, 16, 16, 16, 15, 15, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 11, 11, 11, 11, 11, 13, 13, 13, 13, 13, 13, 13, 13, 13, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 13, 13, 13, 16, 16, 16, 16, 2, 2, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 16, 16, 14, 14, 14, 14, 15, 15, 2, 2, 2, 2, 2, 2, 2, 35, 35, 35, 35, 35, 2, 2, 16, 16, 15, 15, 15, 2, 2, 2, 2, 2, 2, 2, 13, 13, 13, 13, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13, 13, 14, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 13, 13, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 14, 15, 15, 15, 6, 6, 6, 6, 6, 6, 16, 6, 6, 16, 15, 15, 15, 15, 15, 15, 15, 15, 13, 13, 13, 13, 15, 15, 15, 15, 15, 6, 15, 15, 15, 15, 15, 15, 15, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 10, 2, 2, 2, 2, 3, 3, 3, 3, 3, 16, 16, 16, 16, 16, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 1, 16, 16, 11, 11, 11, 11, 11, 11, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 15, 17, 15, 15, 6, 3, 3, 3, 1, 1, 1, 17, 17, 17, 16, 15, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 15, 15, 15, 17, 15, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 6, 6, 6, 6, 6, 6, 6, 6, 14, 17, 15, 17, 17, 17, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 2, 2, 2, 2, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 16, 15, 16, 16, 16, 16, 16, 16, 2, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 3, 2, 1, 2, 2, 2, 46, 46, 46, 2, 16, 16, 16, 16, 16] [1, 1, 1, 3, 5, 5, 5, 5, 5, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 6, 6, 6, 6, 16, 16, 16, 1, 1, 2, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 6, 6, 6, 6, 6, 15, 15, 15, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 3, 1, 1, 17, 1, 1, 1, 1, 1, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 3, 17, 17, 17, 6, 6, 6, 6, 6, 1, 3, 3, 3, 3, 3, 3, 3, 11, 11, 17, 17, 2, 2, 20, 2, 2, 2, 3, 3, 3, 15, 3, 17, 17, 17, 17, 15, 15, 15, 15, 17, 17, 17, 17, 17, 6, 6, 15, 15, 14, 15, 16, 1, 3, 6, 6, 6, 14, 14, 14, 14, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 16, 16, 15, 15, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 11, 11, 1, 1, 3, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 16, 16, 16, 1, 1, 1, 1, 1, 1, 1, 10, 10, 10, 11, 13, 13, 13, 13, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 3, 17, 17, 1, 1, 1, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 1, 1, 1, 1, 1, 16, 16, 16, 16, 16, 16, 3, 3, 3, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 3, 3, 16, 16, 3, 3, 3, 3, 3, 16, 16, 16, 16, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 17, 17, 17, 6, 6, 16, 16, 16, 16, 6, 6, 6, 6, 1, 6, 6, 6, 6, 16, 16, 16, 16, 16, 15, 6, 6, 6, 6, 6, 6, 3, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 1, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 14, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 6, 6, 6, 6, 1, 1, 3, 3, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1, 11, 11, 11, 11, 11, 11, 11, 13, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 6, 6, 6, 6, 2, 2, 2, 15, 15, 15, 3, 6, 6, 6, 6, 1, 16, 16, 16, 16, 16, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 16, 16, 3, 3, 3, 1, 26, 2, 3, 3, 3, 3, 3, 3, 3, 16, 3, 5, 5, 5, 3, 16, 3, 16, 17, 14, 6, 6, 14, 14, 17, 17, 17, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 15, 15, 17, 3, 15, 16, 15, 6, 17, 17, 17, 17, 17, 17, 17, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 1, 1, 1, 1, 1, 15, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 3, 3, 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 3, 9, 6, 6, 6, 6, 6, 1, 3, 3, 3, 3, 10, 10, 10, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 2, 2, 2, 2, 14, 14, 14, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 2, 2, 13, 13, 13, 13, 13, 13, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 16, 16, 16, 2, 2, 2, 2, 2, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 16, 16, 16, 16, 16, 16, 17, 6, 15, 15, 17, 17, 5, 5, 5, 5, 35, 6, 10, 10, 10, 10, 10, 10, 10, 10, 16, 16, 16, 16, 16, 16, 11, 11, 11, 16, 13, 13, 13, 13, 14, 14, 14, 14, 14, 11, 11, 11, 11, 11, 11, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 15, 14, 14, 14, 14, 14, 16, 16, 16, 16, 17, 17, 17, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 13, 13, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 1, 1, 1, 1, 1, 1, 1, 2, 2, 17, 17, 17, 2, 2, 2, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 1, 3, 17, 1, 2, 2, 2, 3, 1, 1, 1, 1, 2, 3, 3, 14, 14, 10, 16, 16, 16, 16, 16, 17, 17, 17, 1, 3, 1, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 6, 6, 2, 2, 2, 2, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 3, 3, 15, 1, 1, 1, 1, 1, 1, 1, 1, 15, 15, 15, 15, 3, 17, 17, 17, 17, 17, 17, 6, 17, 1, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 15, 15, 15, 15, 3, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 5, 5, 5, 5, 14, 14, 3, 3, 3, 3, 17, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 24, 24, 24, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 15, 15, 16, 16, 16, 15, 15, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 11, 11, 11, 11, 11, 13, 13, 13, 13, 13, 13, 13, 13, 13, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 13, 13, 13, 16, 16, 16, 16, 2, 2, 16, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 16, 16, 16, 14, 14, 14, 14, 15, 15, 2, 2, 2, 2, 2, 2, 2, 16, 35, 35, 35, 35, 2, 2, 16, 16, 15, 15, 15, 2, 2, 2, 2, 2, 2, 2, 13, 13, 13, 13, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 13, 13, 13, 13, 13, 13, 14, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 13, 13, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 14, 15, 15, 15, 6, 6, 6, 6, 6, 6, 16, 6, 6, 16, 15, 15, 15, 15, 15, 15, 15, 15, 13, 13, 13, 13, 15, 15, 15, 15, 15, 6, 15, 15, 15, 15, 15, 15, 15, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 2, 10, 2, 2, 2, 2, 3, 3, 3, 3, 3, 16, 16, 16, 16, 16, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 1, 16, 16, 11, 11, 11, 11, 11, 11, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 15, 17, 15, 15, 6, 3, 3, 3, 1, 1, 1, 17, 17, 17, 16, 15, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 3, 3, 3, 15, 15, 15, 17, 15, 1, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 3, 6, 6, 6, 6, 6, 6, 6, 6, 14, 17, 15, 17, 17, 17, 2, 2, 2, 2, 2, 15, 15, 15, 15, 15, 2, 2, 2, 2, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 16, 15, 16, 16, 16, 16, 16, 16, 2, 2, 2, 2, 2, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 3, 2, 1, 2, 2, 2, 46, 46, 46, 2, 16, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "ide_input = [i[0] for i in ide_input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "51d0c913",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_26\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_78\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_26\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=int32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [235]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(ide_output)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n\u001b[1;32m----> 4\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-validated accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score)\n",
      "Input \u001b[1;32mIn [231]\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(model, X, y, k)\u001b[0m\n\u001b[0;32m     26\u001b[0m         y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#         y_train = y_train.reshape(-1)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m         score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     30\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(score[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filekl5a5p_m.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\msok0\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_26\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_78\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_26\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=int32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(ide_input)\n",
    "y = np.asarray(ide_output)\n",
    "model = build_model()\n",
    "score = train_and_evaluate_model(model, X, y)\n",
    "print(\"Cross-validated accuracy:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367e795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
